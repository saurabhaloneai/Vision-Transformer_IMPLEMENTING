{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9795f72-017e-4f6c-8e5a-4ced0ddf7224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#step-1\n",
    "Set patch\n",
    "patch_size = 16\n",
    "#step-2. Print shape of original image tensor and get the image dimensions\n",
    "\"print(f\"Image tensor shape: (image. shape)\")\n",
    "height, width = image. shape[1], image. shape [2]\n",
    "\n",
    "#styep-3. Get image tensor and add batch dimension\n",
    "x = image. unsqueeze (0)\n",
    "print (f\"shape: {x.shape}\")\n",
    "\n",
    "#step-4. Create patch embedding layer(ViT-base)\n",
    "patch_embedding_layer = PatchEmbedding (in_channels=3,\n",
    "patch_size=patch_size,\n",
    "embedding_dim=768) \n",
    "#step-5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fb675b-d3fb-483a-8c73-a6cdfcb87a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding. shape [0]\n",
    "\n",
    "embedding_dimension = patch_embedding. shape(-1]\n",
    "\n",
    "class_token = nn. Parameter (torch. ones (batch_size, 1, embedding_dimension),\n",
    "                            requires_grad=True) #learnable\n",
    "                                             \n",
    "print(f\"Class token embedding shape: {class_token. shape]\")\n",
    "\n",
    "#7. Prepend class_token embedding to patch embedding\n",
    "\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "\n",
    "print(f\"shape : {patch_embedding_class_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fc206-eedb-41fe-8e83-daa85c1772c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn. Parameter (torch. ones (1, number_of_patches+1,\n",
    "requires_grad=True) #learnable\n",
    "embedding_dimension)\n",
    "\n",
    "9. Add position embedding to patch embedding with class_token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and\n",
    "position embedding shape: (patch_and_position_embedding. shape]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f00063-d98f-403f-b930-931107285e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fe1b9-ec43-402c-8d42-56fdb072fb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
